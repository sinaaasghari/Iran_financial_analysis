{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "B3o6dgYaFbDk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from scipy import sparse\n",
        "!pip install kneed\n",
        "from kneed import KneeLocator"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Reading Data"
      ],
      "metadata": {
        "id": "EPYJVhyr5zGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_years_df = pd.read_excel('/content/drive/MyDrive/Clean Data/KMeans Data/finall_v_income_cost.xlsx', usecols=['totall_income','totall_cost'])\n",
        "all_years_df.rename(columns={'totall_income':'Incomes','totall_cost':'Costs'},inplace=True)\n",
        "all_years_df.fillna(0,inplace=True)\n",
        "all_years_df['Incomes'] = all_years_df['Incomes'].astype('int')"
      ],
      "metadata": {
        "id": "RYp0Js6_Jp42"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##DBSCAN using all of data"
      ],
      "metadata": {
        "id": "qAFW-w4Z54cj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_clusters = []\n",
        "for min_pts in range(2, 21):\n",
        "    nn = NearestNeighbors(n_neighbors=min_pts).fit(all_years_df)\n",
        "    neigh_dist, _ = nn.kneighbors()\n",
        "    borders = np.sort(neigh_dist[:,-1])\n",
        "    knee_loc = KneeLocator(np.arange(len(borders)),borders,curve='convex')\n",
        "    eps = borders[knee_loc.elbow]\n",
        "    dbscan = DBSCAN(eps=eps, min_samples=min_pts)\n",
        "    clusters = dbscan.fit_predict(all_years_df)\n",
        "    print(min_pts,np.unique(clusters,return_counts=True),eps)\n",
        "    clusters_num = np.unique(clusters)\n",
        "    all_clusters.append((min_pts,eps,clusters_num))"
      ],
      "metadata": {
        "id": "E29OpDUH5-Vp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running the cell above cause the system to crash because DBSCAN is so computationally expensive"
      ],
      "metadata": {
        "id": "ifQmXOFv6FKx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scaling Data"
      ],
      "metadata": {
        "id": "WsxVaNwn6S4j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "scaled_data = pd.DataFrame(scaler.fit_transform(all_years_df[['Incomes', 'Costs']]),columns=all_years_df.columns)"
      ],
      "metadata": {
        "id": "CAZhNONdvTDG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_clusters = []\n",
        "for min_pts in range(2, 21):\n",
        "    nn = NearestNeighbors(n_neighbors=min_pts).fit(scaled_data)\n",
        "    neigh_dist, _ = nn.kneighbors()\n",
        "    borders = np.sort(neigh_dist[:,-1])\n",
        "    knee_loc = KneeLocator(np.arange(len(borders)),borders,curve='convex')\n",
        "    eps = borders[knee_loc.elbow]\n",
        "    dbscan = DBSCAN(eps=eps, min_samples=min_pts)\n",
        "    clusters = dbscan.fit_predict(scaled_data)\n",
        "    print(min_pts,np.unique(clusters,return_counts=True))\n",
        "    clusters_num = np.unique(clusters)\n",
        "    all_clusters.append((min_pts,eps,clusters_num))"
      ],
      "metadata": {
        "id": "lRt0nHTp6fyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even after scaling the data , the session crashed."
      ],
      "metadata": {
        "id": "ABOPXN5k7Hbc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sampling Data"
      ],
      "metadata": {
        "id": "Lc_yrIYF7WQ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_sampled = scaled_data.sample(n=25000, random_state=42)"
      ],
      "metadata": {
        "id": "eT_Z3fw330d_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_clusters = []\n",
        "for min_pts in range(2, 21):\n",
        "    nn = NearestNeighbors(n_neighbors=min_pts).fit(data_sampled)\n",
        "    neigh_dist, _ = nn.kneighbors()\n",
        "    borders = np.sort(neigh_dist[:,-1])\n",
        "    knee_loc = KneeLocator(np.arange(len(borders)),borders,curve='convex')\n",
        "    eps = borders[knee_loc.elbow]\n",
        "    dbscan = DBSCAN(eps=eps, min_samples=min_pts)\n",
        "    clusters = dbscan.fit_predict(data_sampled)\n",
        "    print(min_pts,np.unique(clusters,return_counts=True))\n",
        "    clusters_num = np.unique(clusters)\n",
        "    all_clusters.append((min_pts,eps,clusters_num))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZlbRPHQ7ibT",
        "outputId": "96117919-690a-472f-8c91-6b3f40a8f37f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2 (array([-1,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([   32, 24936,     2,     2,     2,     4,     3,     2,     2,\n",
            "           6,     2,     5,     2]))\n",
            "3 (array([-1,  0,  1,  2]), array([   23, 24968,     6,     3]))\n",
            "4 (array([-1,  0,  1]), array([   46, 24949,     5]))\n",
            "5 (array([-1,  0,  1]), array([   46, 24949,     5]))\n",
            "6 (array([-1,  0,  1]), array([   75, 24920,     5]))\n",
            "7 (array([-1,  0]), array([   39, 24961]))\n",
            "8 (array([-1,  0]), array([   67, 24933]))\n",
            "9 (array([-1,  0]), array([   67, 24933]))\n",
            "10 (array([-1,  0]), array([   64, 24936]))\n",
            "11 (array([-1,  0]), array([   69, 24931]))\n",
            "12 (array([-1,  0]), array([   68, 24932]))\n",
            "13 (array([-1,  0]), array([   55, 24945]))\n",
            "14 (array([-1,  0]), array([   61, 24939]))\n",
            "15 (array([-1,  0]), array([   52, 24948]))\n",
            "16 (array([-1,  0]), array([  114, 24886]))\n",
            "17 (array([-1,  0]), array([  111, 24889]))\n",
            "18 (array([-1,  0]), array([   53, 24947]))\n",
            "19 (array([-1,  0]), array([   52, 24948]))\n",
            "20 (array([-1,  0]), array([  125, 24875]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally We're getting some results but they are both meaningless and worthless"
      ],
      "metadata": {
        "id": "SZwajgcy71Z4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sparsing Data"
      ],
      "metadata": {
        "id": "SEVzr9ol8FRp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sparse_matrix = sparse.csr_matrix(data_sampled)\n",
        "sparse_matrix = pd.DataFrame(scaler.fit_transform(data_sampled[['Incomes', 'Costs']]),columns=all_years_df.columns)"
      ],
      "metadata": {
        "id": "GVSFqfS7wFY8"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_clusters = []\n",
        "for min_pts in range(2, 21):\n",
        "    nn = NearestNeighbors(n_neighbors=min_pts).fit(sparse_matrix)\n",
        "    neigh_dist, _ = nn.kneighbors()\n",
        "    borders = np.sort(neigh_dist[:,-1])\n",
        "    knee_loc = KneeLocator(np.arange(len(borders)),borders,curve='convex')\n",
        "    eps = borders[knee_loc.elbow]\n",
        "    dbscan = DBSCAN(eps=eps, min_samples=min_pts)\n",
        "    clusters = dbscan.fit_predict(sparse_matrix)\n",
        "    print(min_pts,np.unique(clusters,return_counts=True))\n",
        "    clusters_num = np.unique(clusters)\n",
        "    all_clusters.append((min_pts,eps,clusters_num))"
      ],
      "metadata": {
        "id": "IGtsOrm_KCIB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45a0d802-61f3-40b1-bc89-5a18a1c60b4c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2 (array([-1,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,\n",
            "       16, 17, 18, 19, 20]), array([   63, 24884,     2,     3,     2,     3,     2,     2,     3,\n",
            "           2,     6,     2,     3,     4,     2,     3,     4,     2,\n",
            "           2,     2,     2,     2]))\n",
            "3 (array([-1,  0,  1,  2,  3,  4,  5,  6,  7]), array([   98, 24872,     3,     5,     5,     3,     6,     4,     4]))\n",
            "4 (array([-1,  0]), array([   45, 24955]))\n",
            "5 (array([-1,  0,  1]), array([   30, 24965,     5]))\n",
            "6 (array([-1,  0]), array([   45, 24955]))\n",
            "7 (array([-1,  0]), array([   42, 24958]))\n",
            "8 (array([-1,  0]), array([   56, 24944]))\n",
            "9 (array([-1,  0]), array([   66, 24934]))\n",
            "10 (array([-1,  0]), array([   58, 24942]))\n",
            "11 (array([-1,  0]), array([   39, 24961]))\n",
            "12 (array([-1,  0]), array([  112, 24888]))\n",
            "13 (array([-1,  0]), array([   61, 24939]))\n",
            "14 (array([-1,  0]), array([   64, 24936]))\n",
            "15 (array([-1,  0]), array([   41, 24959]))\n",
            "16 (array([-1,  0]), array([   53, 24947]))\n",
            "17 (array([-1,  0]), array([   53, 24947]))\n",
            "18 (array([-1,  0]), array([   29, 24971]))\n",
            "19 (array([-1,  0]), array([   55, 24945]))\n",
            "20 (array([-1,  0]), array([  118, 24882]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again the results are meaningless and worthless."
      ],
      "metadata": {
        "id": "YP-7_2IK9Rdg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##In conclusion the DBSCAN is not a suitable algorithm for our dataset and we should stick to KMeans algorithm"
      ],
      "metadata": {
        "id": "nIMGISjk9XAw"
      }
    }
  ]
}